{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"The Nimbus Portal Client Guide","text":"<p>The Nimbus Portal client is the Nimbus client implementation of the Portal network specifications.</p> <p>The Portal Network aims to deliver a reliable and decentralized access to the Ethereum historical block data.</p> <p>This book describes how to build, run and monitor the Nimbus Portal client, and how to use and test its currently implemented functionality.</p> <p>To quickly get your Nimbus Portal client up and running, follow the quickstart page:</p> <ul> <li>Quickstart for Linux / macOS users</li> <li>Quickstart for Windows users</li> <li>Quickstart for Docker users</li> </ul>"},{"location":"index.html#development-status","title":"Development status","text":"<p>The development of this client is on par with the latest Portal specifications and will continue to evolve according to the Portal specifications.</p> <p>The Portal history sub-network is already operational on the Portal mainnet.</p> <p>The Nimbus Portal client is default ran on the Portal mainnet but can also be run on a (local) testnet.</p>"},{"location":"index.html#supported-sub-networks-and-content","title":"Supported sub-networks and content","text":"<ul> <li> <p>History network: block bodies and receipts</p> </li> <li> <p>Beacon network: consensus light client data and historical summaries. This network is experimental and default disabled.</p> </li> </ul>"},{"location":"index.html#supported-functionality","title":"Supported functionality","text":"<ul> <li>Portal JSON-RPC API</li> <li>Consensus light client sync through content available on the Portal beacon network.</li> </ul>"},{"location":"index.html#get-in-touch","title":"Get in touch","text":"<p>Need help with anything? Join us on Status and Discord.</p>"},{"location":"index.html#donate","title":"Donate","text":"<p>If you'd like to contribute to Nimbus development:</p> <ul> <li>Our donation address is <code>0xDeb4A0e8d9a8dB30a9f53AF2dCc9Eb27060c6557</code></li> <li>We're also listed on GitCoin</li> </ul>"},{"location":"index.html#disclaimer","title":"Disclaimer","text":"<p>This documentation assumes Nimbus Portal client is in its ideal state. The project is still under heavy development. Please submit a Github issue if you come across a problem.</p>"},{"location":"access-content.html","title":"Access content on the Portal network","text":"<p>Once you have a Portal node connected to the network with the JSON-RPC interface enabled, then you can access the content available on the Portal network.</p> <p>You can for example access execution layer blocks through the standardized Portal JSON-RPC call <code>portal_historyGetContent</code>:</p> <pre><code>CONTENTKEY=0x004e61bc0000000000 # Serialized content key for block body of block 12345678\n# Run this command to get this block body (unverified):\ncurl -s -X POST -H 'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"id\":\"1\",\"method\":\"portal_historyGetContent\",\"params\":[\"'${CONTENTKEY}'\"]}' http://localhost:8545\n</code></pre>"},{"location":"adding-documentation.html","title":"Adding documentation","text":"<p>The documentation visible on https://fluffy.guide is generated with mkdocs.</p> <p>If you want to be able to dry run any changes you make, you best install mkdocs locally.</p> <p>All the documentation related files can be found under the <code>./portal/docs/the_fluffy_book</code> directory.</p>"},{"location":"adding-documentation.html#how-to-test-and-add-documentation-changes","title":"How to test and add documentation changes","text":"<ul> <li>Install <code>mkdocs</code></li> <li>Install Material for MkDocs by running <code>pip install mkdocs-material mkdocs-mermaid2-plugin</code>.</li> <li>Make your changes to the documentation</li> <li>Run <code>mkdocs serve</code> from the <code>./portal/docs/the_fluffy_book</code> directory and test your changes. Alter as required.</li> <li>Push your changes to a PR on nimbus-eth1</li> </ul> <p>When the PR gets merged, a CI job will run that deploys automatically the changes to https://fluffy.guide.</p>"},{"location":"architecture.html","title":"Nimbus Portal Client Architecture","text":"<p>This section outlines the Nimbus Portal client's architecture and shows the main components in the codebase. The arrows indicate a dependancy relationship between each component.</p>"},{"location":"architecture.html#nimbus-portal-client-high-level-architecture","title":"Nimbus Portal client high level architecture","text":"<p>This diagram outlines the Nimbus Portal client high-level architecture. <pre><code>\ngraph TD;\n    nimbus_portal_client ---&gt; id2(PortalNode) &amp; id5(MetricsHttpServer)\n    nimbus_portal_client ---&gt; id3(RpcHttpServer) &amp; id4(RpcWebSocketServer)\n    id3(RpcHttpServer) &amp; id4(RpcWebSocketServer) &amp; id2(PortalNode) ---&gt; id7(BeaconNetwork)\n    id3(RpcHttpServer) &amp; id4(RpcWebSocketServer) &amp; id2(PortalNode) ----&gt; id8(HistoryNetwork)\n    id2(PortalNode) --&gt; id10(Discv5Protocol)\n</code></pre></p> <p>When the Nimbus Portal client starts it runs an instance of <code>PortalNode</code> which manages the <code>Discv5Protocol</code>, <code>BeaconNetwork</code> and <code>HistoryNetwork</code> instances. There is a single instance of each of these components and each of the subnetwork instances can be enabled/disabled depending on the startup configuration selected. The <code>PortalNode</code> instance includes everything needed to participate in the Portal network to enable storage of offered content and serving content requests from other Portal nodes. It may become part of a library in the future which would allow other projects to easily embed an instance of the Nimbus Portal client in their codebase.</p> <p>The <code>RpcHttpServer</code> and <code>RpcWebSocketServer</code> enable serving JSON-RPC requests from Portal network over HTTP and WebSocket respectively.</p>"},{"location":"architecture.html#portal-subnetworks","title":"Portal subnetworks","text":"<p>This diagram outlines the generic architecture of each Portal subnetwork.</p> <pre><code>\ngraph TD;\n    PortalSubnetwork --&gt; id1(PortalProtocol)\n    PortalSubnetwork --&gt; id2(ContentQueue)\n    id1(PortalProtocol) --&gt; id3(Discv5Protocol)\n    id1(PortalProtocol) ---&gt; id4(RoutingTable)\n    id1(PortalProtocol) --&gt; id5(RadiusCache)\n    id1(PortalProtocol) ---&gt; id6(OfferCache)\n    id1(PortalProtocol) --&gt; id7(ContentCache)\n    id1(PortalProtocol) ---&gt; id8(ContentDb)\n    id1(PortalProtocol) --&gt; id9(OfferQueue)\n    id1(PortalProtocol) --&gt; id10(PortalStream)\n    id10(PortalStream) --&gt; id11(UtpDiscv5Protocol)\n    id10(PortalStream) --&gt; id2(ContentQueue)\n    id11(UtpDiscv5Protocol) --&gt; id3(Discv5Protocol)\n</code></pre> <p>There are some differences between each of the subnetworks but the components used in each are mostly the same. Only the <code>Discv5Protocol</code> and <code>ContentDb</code> instances are shared between the Portal subnetworks while the other components have separate instances per subnetwork.</p> <p>The <code>Discv5Protocol</code> type implements the Discv5 protocol which is used as a transport to send messages between Portal nodes. Each Portal subnetwork (such as the <code>BeaconNetwork</code> and <code>HistoryNetwork</code>) holds an instance of <code>PortalProtocol</code> which implements the Portal Wire protocol and an instance of <code>ContentQueue</code> which receives Portal content from the <code>PortalStream</code> when the node receives content from peers. When a content transfer is initiated which is bigger than the max Discv5 message size, then the <code>PortalStream</code> transfers the content using the <code>UtpDiscv5Protocol</code> type which implements uTP on top of Discv5.</p> <p>The <code>RoutingTable</code> implements a Kademlia based DHT which holds the peer ENRs which the Portal node discovers while participating in each of the Portal Wire subprotocols. The <code>RadiusCache</code> holds the last known radius for each peer which is collected when pinging each node in the routing table periodically. The <code>OfferCache</code> caches the content ids of the most recent content successfully offered and stored so that the Portal node can reject content that it already has without doing a database lookup. The <code>ContentCache</code> improves the performance of content lookups (used by the JSON-RPC API's) by caching the most recently fetched content in a LRU cache.</p> <p>The <code>ContentDb</code> is the main database in the Nimbus Portal client which internally uses sqlite to store the content data on disk. The <code>PortalProtocol</code> uses the <code>OfferQueue</code> to hold pending offer requests which are passed to the <code>PortalStream</code> by the concurrent offer workers which run as a part of <code>PortalProtocol</code>.</p>"},{"location":"basics-for-developers.html","title":"The basics for developers","text":"<p>When working on Nimbus Portal client in the nimbus-eth1 repository, you can run the <code>env.sh</code> script to run a command with the right environment variables set. This means the vendored Nim and Nim modules will be used, just as when you use <code>make</code>.</p> <p>E.g.:</p> <pre><code># start a new interactive shell with the right env vars set\n./env.sh bash\n</code></pre> <p>More development tips can be found on the general nimbus-eth1 readme.</p> <p>The code follows the Status Nim Style Guide.</p>"},{"location":"basics-for-developers.html#nim-code-formatting","title":"Nim code formatting","text":"<p>The <code>portal</code> codebase is formatted with nph. Check out the this page on how to install nph.</p> <p>The Nimbus Portal CI tests check the code formatting according to the style rules of nph. Developers will need to make sure the code changes in PRs are formatted as such.</p> <p>Note</p> <p>In the future the nph formatting might be added within the build environment make targets or similar, but currently it is a manual step that developers will need to perform.</p>"},{"location":"beacon-content-bridging.html","title":"Bridging content into the Portal beacon network","text":""},{"location":"beacon-content-bridging.html#seeding-from-content-bridges","title":"Seeding from content bridges","text":"<p>Run a Nimbus Portal client with the JSON-RPC API enabled.</p> <pre><code>./build/nimbus_portal_client --rpc\n</code></pre> <p>Build &amp; run the Nimbus Portal bridge for the beacon network: <pre><code>make nimbus_portal_bridge\n\nTRUSTED_BLOCK_ROOT=0x1234567890123456789012345678901234567890123456789012345678901234 # Replace with trusted block root.\n# --rest-url = access to beacon node API, default http://127.0.0.1:5052\n# --portal-rpc=url = access to the Portal node API, default http://127.0.0.1:8545\n./build/nimbus_portal_bridge beacon --trusted-block-root:${TRUSTED_BLOCK_ROOT} --rest-url:http://127.0.0.1:5052 --portal-rpc-url:http://127.0.0.1:8545\n</code></pre></p> <p>The Nimbus Portal bridge will connect to the Nimbus Portal client over the JSON-RPC interface and start gossiping an <code>LightClientBootstrap</code> for given trusted block root and gossip backfill <code>LightClientUpdate</code>s.</p> <p>Next, it will gossip a new <code>LightClientOptimisticUpdate</code>, <code>LightClientFinalityUpdate</code> and <code>LightClientUpdate</code> as they become available.</p>"},{"location":"build-from-source.html","title":"Build from source","text":"<p>Building the Nimbus Portal client from source ensures that all hardware-specific optimizations are turned on. The build process itself is simple and fully automated, but may take a few minutes.</p> <p>Nim</p> <p>The Nimbus Portal client is written in the Nim programming language. The correct version will automatically be downloaded as part of the build process!</p>"},{"location":"build-from-source.html#prerequisites","title":"Prerequisites","text":"<p>Make sure you have all needed prerequisites.</p>"},{"location":"build-from-source.html#building-the-nimbus-portal-client","title":"Building the Nimbus Portal client","text":""},{"location":"build-from-source.html#1-clone-the-nimbus-eth1-repository","title":"1. Clone the <code>nimbus-eth1</code> repository","text":"<pre><code>git clone https://github.com/status-im/nimbus-eth1.git\ncd nimbus-eth1\n</code></pre>"},{"location":"build-from-source.html#2-run-the-nimbus-portal-client-build-process","title":"2. Run the Nimbus Portal client build process","text":"<p>To build the Nimbus Portal client and its dependencies, run:</p> <pre><code>make nimbus_portal_client\n</code></pre> <p>This step can take several minutes. After it has finished, you can check if the build was successful by running:</p> <pre><code># See available command line options\n./build/nimbus_portal_client --help\n</code></pre> <p>If you see the command-line options, your installation was successful! Otherwise, don't hesitate to reach out to us in the <code>#nimbus-portal</code> channel of our discord.</p>"},{"location":"build-from-source.html#keeping-the-nimbus-portal-client-updated","title":"Keeping the Nimbus Portal client updated","text":"<p>When you decide to upgrade the Nimbus Portal client to a newer version, make sure to follow the how to upgrade page.</p>"},{"location":"connect-to-portal.html","title":"Connect to the Portal network","text":"<p>Connecting to the current Portal network is as easy as running following command:</p> <pre><code>./build/nimbus_portal_client --rpc\n</code></pre> <p>This will connect to the public Portal mainnet which contains nodes of the different clients.</p> <p>Note</p> <p>By default the Portal node will connect to the bootstrap nodes of the public mainnet.</p> <p>When testing locally the <code>--network:none</code> option can be provided to avoid connecting to any of the default bootstrap nodes.</p> <p>The <code>--rpc</code> option will also enable the different JSON-RPC interfaces through which you can access the Portal Network.</p> <p>The Nimbus Portal client fully supports the Portal Network JSON-RPC Specification.</p>"},{"location":"db_pruning.html","title":"Database pruning","text":"<p>Default the Nimbus Portal client runs with a specific storage capacity (<code>--storage-capacity=x</code>, default set to 2GB). This means that the node's radius is dynamically adjusted to not exceed the configured capacity. As soon as the storage capacity is to be exceeded the pruning of content takes place and a new smaller radius is set.</p> <p>As long as the configured storage capacity remains the same, pruning is done automatically.</p> <p>In case the storage capacity of a Nimbus Portal client is changed, a manual step might be required. There are two scenarios possible: - Adjusting to a higher storage capacity - Adjusting to a lower storage capacity</p>"},{"location":"db_pruning.html#adjusting-to-a-higher-storage-capacity","title":"Adjusting to a higher storage capacity","text":"<p>This requires no manual steps as no pruning will be required. On the restart of the Nimbus Portal client with a higher configured storage capacity, the initial radius will be increased to the maximum radius until the new storage capacity is reached. Then the automatic pruning will take place and the radius will be decreased.</p>"},{"location":"db_pruning.html#adjusting-to-a-lower-storage-capacity","title":"Adjusting to a lower storage capacity","text":"<p>When a Nimbus Portal client is restarted with a lower storage capacity, pruning will take place automatically. The database will be pruned in intervals until the storage drops under the newly configured storage capacity. The radius will also be adjusted with each pruning cycle.</p> <p>However, on disk the database will not lower in size. This is because empty pages are kept in the SQL database until a vacuum command is done. To do this you can run the <code>--force-prune</code> option at start-up. Note that this will temporarily double the database storage capacity as a temporary copy of the database needs to be made. Because of this, the vacuum is not executed automatically but requires you to manually enable the <code>--force-prune</code> flag.</p> <p>You can also use the <code>fcli_db</code> tool its  <code>prune</code> command on the database directly to force this vacuuming.</p> <p>Another simple but more drastic solution is to delete the <code>db</code> subdirectory in the <code>--data-dir</code> provided to your Nimbus Portal client. This will start your Nimbus Portal client with a fresh database.</p>"},{"location":"eth-data-exporter.html","title":"Exporting Ethereum content for Portal","text":""},{"location":"eth-data-exporter.html#eth_data_exporter","title":"eth_data_exporter","text":"<p>The <code>eth_data_exporter</code> is a tool to extract content from Ethereum EL or CL and prepare it as Portal content and content keys.</p> <p>The <code>eth_data_exporter</code> can export data for different Portal networks. Currently the <code>history</code> and the <code>beacon</code> networks are supported.</p> <p>Example commands:</p> <pre><code># Build the tool\nmake eth_data_exporter\n# See the different commands and options\n./build/eth_data_exporter --help\n</code></pre> <pre><code># Request of `BeaconLightClientUpdate`s and export into the Portal\n# network supported format\n./build/eth_data_exporter beacon exportLCUpdates --rest-url:http://testing.mainnet.beacon-api.nimbus.team --start-period:816 --count:4\n</code></pre>"},{"location":"history-content-bridging.html","title":"Bridging content into the Portal history network","text":""},{"location":"history-content-bridging.html#seeding-history-content-with-the-nimbus-portal-bridge","title":"Seeding history content with the Nimbus Portal bridge","text":"<p>The Nimbus Portal bridge requires <code>era1</code> files as source for the block content from before the merge. It requires access to a full node with EL JSON-RPC API for seeding the latest (head of the chain) block content. Any block content between the merge and the latest is currently not implemented, but will be implemented in the future by usage of <code>era</code> files as source.</p>"},{"location":"history-content-bridging.html#step-1-run-a-portal-client","title":"Step 1: Run a Portal client","text":"<p>Run a Portal client with the Portal JSON-RPC API enabled, e.g. Nimbus Portal client:</p> <pre><code>./build/nimbus_portal_client --rpc --storage-capacity:0\n</code></pre> <p>Note: The <code>--storage-capacity:0</code> option is not required, but it is added here for the use case where the node's only focus is on gossiping content from the portal bridge.</p>"},{"location":"history-content-bridging.html#step-2-run-an-el-client-optional","title":"Step 2: Run an EL client (Optional)","text":"<p>To seed the latest blocks, the Nimbus Portal bridge needs access to the EL JSON-RPC API, either through a local Ethereum client or via a web3 provider.</p>"},{"location":"history-content-bridging.html#step-3-run-the-portal-bridge-in-history-mode","title":"Step 3: Run the Portal bridge in history mode","text":"<p>Build &amp; run the Nimbus Portal bridge: <pre><code>make nimbus_portal_bridge\n\nWEB3_URL=\"http://127.0.0.1:8548\" # Replace with your provider.\n./build/nimbus_portal_bridge history --era1-dir:/somedir/era1/ --web3-url:${WEB3_URL}\n</code></pre></p> <p>By default the Nimbus Portal bridge runs in <code>--backfill-mode:regular</code>. This means the bridge will access era1 files to gossip block bodies and receipts into the network. There are other backfill modes available which first try to download the data from the network before gossiping it into the network, e.g. sync and audit mode.</p> <p>Example: regular backfill (gossip everything): <pre><code>./build/nimbus_portal_bridge history --backfill-mode:regular --era1-dir:/somedir/era1/\n</code></pre></p> <p>Example: audit-mode backfill (random sample &amp; gossip missing): <pre><code>./build/nimbus_portal_bridge history --backfill-mode:audit --era1-dir:/somedir/era1/\n</code></pre></p> <p>It is also possible to enable <code>--latest</code> mode, which means that the latest block content will be gossiped into the network.</p> <pre><code>WEB3_URL=\"http://127.0.0.1:8548\" # Replace with your provider.\n./build/nimbus_portal_bridge history --latest:true --backfill-mode:none --era1-dir:/somedir/era1/ --web3-url:${WEB3_URL}\n</code></pre>"},{"location":"metrics.html","title":"Metrics and their visualisation","text":"<p>In this page we'll cover how to enable metrics and how to use Grafana and Prometheus to help you visualize these real-time metrics concerning the Portal node.</p>"},{"location":"metrics.html#enable-metrics-in-the-nimbus-portal-client","title":"Enable metrics in the Nimbus Portal client","text":"<p>To enable metrics run the Nimbus Portal client with the <code>--metrics</code> flag: <pre><code>./build/nimbus_portal_client --metrics\n</code></pre> Default the metrics are available at http://127.0.0.1:8008/metrics.</p> <p>The address can be changed with the <code>--metrics-address</code> and <code>--metrics-port</code> options.</p> <p>This provides only a snapshot of the current metrics. In order track the metrics over time and to also visualise them one can use for example Prometheus and Grafana.</p>"},{"location":"metrics.html#visualisation-through-prometheus-and-grafana","title":"Visualisation through Prometheus and Grafana","text":"<p>The steps on how to set up metrics visualisation with Prometheus and Grafana is explained in this guide.</p> <p>A Nimbus Portal specific dashboard can be found here.</p> <p>This is the dashboard used for our Nimbus Portal network fleet. In order to use it locally, you will have to remove the <code>{job=\"nimbus-fluffy-metrics\"}</code> part from the <code>instance</code> and <code>container</code> variables queries in the dashboard settings. Or they can also be changed to a constant value.</p> <p>The other option would be to remove those variables and remove their usage in each panel query.</p>"},{"location":"nimbus-portal-with-hive.html","title":"Testing Nimbus Portal client with hive","text":"<p>The <code>nimbus_portal_client</code> is one of the Portal clients that is being tested with hive.</p> <p>To see the status of the tests for the current version you can access https://portal-hive.ethdevops.io/.</p>"},{"location":"nimbus-portal-with-hive.html#run-the-hive-tests-locally","title":"Run the hive tests locally","text":"<p>Build hive:</p> <pre><code>git clone https://github.com/ethereum/hive.git\ncd ./hive\ngo build .\n</code></pre> <p>Example commands for running test suites:</p> <pre><code># Run the portal hive tests with only the Nimbus Portal client\n./hive --sim portal --client nimbus-portal\n\n# Run the portal hive tests with different clients\n./hive --sim portal --client nimbus-portal,trin,ultralight,shisui\n\n# Run portal hive tests from a specific portal hive simulator\n./hive --sim portal --client nimbus-portal --sim.limit history-interop\n</code></pre> <p>Access the results through web-ui:</p> <pre><code>go build ./cmd/hiveview\n./hiveview --serve --logdir ./workspace/logs\n</code></pre> <p>Note</p> <p>You can see all the implemented Portal simulators in https://github.com/ethereum/hive/blob/master/simulators/portal/</p>"},{"location":"nimbus-portal-with-hive.html#build-a-local-development-docker-image-for-hive","title":"Build a local development Docker image for hive","text":"<p>To debug &amp; develop the Nimbus Portal client code against hive tests you might want to create a local development Docker image.</p> <p>To do that follow next steps:</p> <p>1) Clone and build hive, see above.</p> <p>2) Build the local development Docker image using the following command: <pre><code>docker build --tag nimbus-portal-dev --file ./portal/docker/Dockerfile.debug .\n</code></pre></p> <p>3) Modify the <code>FROM</code> tag in the portal-hive <code>Dockerfile</code> of Nimbus Portal client at <code>./hive/clients/nimbus-portal/Dockerfile</code> to use the image that was build in step 2.</p> <p>4) Run the tests as usual.</p> <p>Warning</p> <p>The <code>./vendors</code> dir is dockerignored and cached. If you have to make local changes to one of the dependencies in that directory you will have to remove <code>vendors/</code> from <code>./portal/docker/Dockerfile.debug.dockerignore</code>.</p> <p>Note</p> <p>When developing on Linux the <code>./portal/docker/Dockerfile.debug.linux</code> Dockerfile can also be used instead. It does require to manually build <code>nimbus_portal_client</code> first as it copies over this binary.</p>"},{"location":"prerequisites.html","title":"Prerequisites","text":"<p>The Nimbus Portal client runs on Linux, macOS, Windows, and Android.</p>"},{"location":"prerequisites.html#build-prerequisites","title":"Build prerequisites","text":"<p>When building from source, you will need additional build dependencies to be installed:</p> <ul> <li>Developer tools (C compiler, Make, Bash, Git 2.9.4 or newer)</li> <li>CMake</li> </ul> LinuxmacOSWindowsAndroid <p>On common Linux distributions the dependencies can be installed with:</p> <pre><code># Debian and Ubuntu\nsudo apt-get install build-essential git cmake\n\n# Fedora\ndnf install @development-tools cmake\n\n# Arch Linux, using an AUR manager\nyourAURmanager -S base-devel cmake\n</code></pre> <p>With Homebrew:</p> <pre><code>brew install cmake\n</code></pre> <p>To build the Nimbus Portal client on Windows, the MinGW-w64 build environment is recommended.</p> <ul> <li> <p>Install Mingw-w64 for your architecture using the \"MinGW-W64 Online Installer\":</p> <ol> <li>Select your architecture in the setup menu (<code>i686</code> on 32-bit, <code>x86_64</code> on 64-bit).</li> <li>Set threads to <code>win32</code>.</li> <li>Set exceptions to \"dwarf\" on 32-bit and \"seh\" on 64-bit.</li> <li>Change the installation directory to <code>C:\\mingw-w64</code> and add it to your system PATH in <code>\"My Computer\"/\"This PC\" -&gt; Properties -&gt; Advanced system settings -&gt; Environment Variables -&gt; Path -&gt; Edit -&gt; New -&gt; C:\\mingw-w64\\mingw64\\bin</code> (<code>C:\\mingw-w64\\mingw32\\bin</code> on 32-bit).</li> </ol> <p>Note</p> <p>If the online installer isn't working you can try installing <code>mingw-w64</code> through MSYS2.</p> </li> <li> <p>Install CMake.</p> </li> <li> <p>Install Git for Windows and use a \"Git Bash\" shell to clone nimbus-eth1 and build <code>nimbus_portal_client</code>.</p> </li> </ul> <ul> <li>Install the Termux app from FDroid or the Google Play store</li> <li>Install a PRoot of your choice following the instructions for your preferred distribution. The Ubuntu PRoot is known to contain all <code>nimbus_portal_client</code> prerequisites compiled on Arm64 architecture (the most common architecture for Android devices).</li> </ul> <p>Assuming you use Ubuntu PRoot:</p> <pre><code>apt install build-essential git\n</code></pre>"},{"location":"protocol-interop-testing.html","title":"Protocol interoperability testing","text":"<p>This document shows a set of commands that can be used to test the individual protocol messages per network (Discovery v5 and Portal networks), e.g. to test client protocol interoperability.</p> <p>Two ways are explained, the first, by keeping a node running and interacting with it through the JSON-RPC service. The second, by running cli applications that attempt to send 1 specific message and then shutdown.</p> <p>The first is more powerful and complete, the second one might be easier to do some quick testing.</p>"},{"location":"protocol-interop-testing.html#run-the-nimbus-portal-client-and-test-protocol-messages-via-json-rpc-api","title":"Run the Nimbus Portal client and test protocol messages via JSON-RPC API","text":"<p>First build the Nimbus Portal client as explained here.</p> <p>Next run it with the JSON-RPC server enabled: <pre><code>./build/nimbus_portal_client --rpc --rpc-api:portal,discovery --bootstrap-node:enr:&lt;base64 encoding of ENR&gt;\n</code></pre></p>"},{"location":"protocol-interop-testing.html#testing-discovery-v5-layer","title":"Testing Discovery v5 layer","text":"<p>Testing the Discovery v5 protocol messages:</p> <pre><code># Ping / Pong\ncurl -s -X POST -H 'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"id\":\"1\",\"method\":\"discv5_ping\",\"params\":[\"enr:&lt;base64 encoding of ENR&gt;\"]}' http://localhost:8545 | jq\n\n# FindNode / Nodes\n# Extra parameter is an array of requested logarithmic distances\ncurl -s -X POST -H 'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"id\":\"1\",\"method\":\"discv5_findNode\",\"params\":[\"enr:&lt;base64 encoding of ENR&gt;\", [254, 255, 256]]}' http://localhost:8545 | jq\n\n# TalkReq / TalkResp\n# Extra parameters are the protocol id and the request byte string, hex encoded.\ncurl -s -X POST -H 'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"id\":\"1\",\"method\":\"discv5_talkReq\",\"params\":[\"enr:&lt;base64 encoding of ENR&gt;\", \"\", \"\"]}' http://localhost:8545 | jq\n\n# Read out the discover v5 routing table contents\ncurl -s -X POST -H 'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"id\":\"1\",\"method\":\"discv5_routingTableInfo\",\"params\":[]}' http://localhost:8545 | jq\n</code></pre>"},{"location":"protocol-interop-testing.html#testing-portal-networks-layer","title":"Testing Portal Networks layer","text":"<p>Testing the Portal wire protocol messages:</p> <pre><code># Ping / Pong\ncurl -s -X POST -H 'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"id\":\"1\",\"method\":\"portal_historyPing\",\"params\":[\"enr:&lt;base64 encoding of ENR&gt;\"]}' http://localhost:8545 | jq\n\n# FindNode / Nodes\n# Extra parameter is an array of requested logarithmic distances\ncurl -s -X POST -H 'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"id\":\"1\",\"method\":\"portal_historyFindNodes\",\"params\":[\"enr:&lt;base64 encoding of ENR&gt;\", [254, 255, 256]]}' http://localhost:8545 | jq\n\n# FindContent / Content\n# A request with an invalid content key will not receive a response\ncurl -s -X POST -H 'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"id\":\"1\",\"method\":\"portal_historyFindContent\",\"params\":[\"enr:&lt;base64 encoding of ENR&gt;\", \"02829bd824b016326a401d083b33d092293333a830d1c390624d3bd4e409a61a858e5dcc5517729a9170d014a6c96530d64dd8621d\"]}' http://localhost:8545 | jq\n\n# Read out the Portal history network routing table contents\ncurl -s -X POST -H 'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"id\":\"1\",\"method\":\"portal_historyRoutingTableInfo\",\"params\":[]}' http://localhost:8545 | jq\n</code></pre> <p>The <code>portal_history_</code> prefix can be replaced for testing other networks such as <code>portal_beacon_</code>.</p>"},{"location":"protocol-interop-testing.html#test-discovery-and-portal-wire-protocol-messages-with-cli-tools","title":"Test Discovery and Portal Wire protocol messages with cli tools","text":""},{"location":"protocol-interop-testing.html#testing-discovery-v5-layer-dcli","title":"Testing Discovery v5 layer: dcli","text":"<pre><code># Build dcli from nim-eth vendor module\n(cd vendor/nim-eth/; ../../env.sh nimble build_dcli)\n</code></pre> <p>With the <code>dcli</code> tool you can test the individual Discovery v5 protocol messages, e.g.:</p> <pre><code># Test Discovery Ping, should print the content of ping message\n./vendor/nim-eth/build/dcli ping enr:&lt;base64 encoding of ENR&gt;\n\n# Test Discovery FindNode, should print the content of the returned ENRs\n# Default a distance of 256 is requested, change this with --distance argument\n./vendor/nim-eth/build/dcli findnode enr:&lt;base64 encoding of ENR&gt;\n\n# Test Discovery TalkReq, should print the TalkResp content\n./vendor/nim-eth/build/dcli talkreq enr:&lt;base64 encoding of ENR&gt;\n</code></pre> <p>Each <code>dcli</code> run will default generate a new network key and thus a new node id and ENR.</p>"},{"location":"protocol-interop-testing.html#testing-portal-networks-layer-portalcli","title":"Testing Portal Networks layer: portalcli","text":"<pre><code># Build portalcli\nmake portalcli\n</code></pre> <p>With the <code>portalcli</code> tool you can test the individual Portal wire protocol messages, e.g.:</p> <pre><code># Test Portal wire Ping, should print the content of ping message\n./build/portalcli ping enr:&lt;base64 encoding of ENR&gt;\n\n# Test Portal wire FindNode, should print the content of the returned ENRs\n# Default a distance of 256 is requested, change this with --distance argument\n./build/portalcli findnodes enr:&lt;base64 encoding of ENR&gt;\n\n# Test Portal wire FindContent, should print the returned content\n./build/portalcli findcontent enr:&lt;base64 encoding of ENR&gt;\n\n# Default the history network is tested, but you can provide another protocol id\n./build/portalcli ping enr:&lt;base64 encoding of ENR&gt; --protocol-id:0x500B\n</code></pre> <p>Each <code>portalcli</code> run will default generate a new network key and thus a new node id and ENR.</p>"},{"location":"quick-start-docker.html","title":"Quick start - Docker","text":"<p>This page takes you through the steps of getting the Nimbus Portal client running on the public network by use of the public Docker image.</p> <p>The Docker image gets rebuild from latest master every night and only <code>amd64</code> is supported currently.</p>"},{"location":"quick-start-docker.html#steps","title":"Steps","text":""},{"location":"quick-start-docker.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> </ul>"},{"location":"quick-start-docker.html#use-the-docker-image-to-run-the-nimbus-portal-client-on-the-portal-network","title":"Use the Docker image to run the Nimbus Portal client on the Portal network","text":"<pre><code># Connect to the Portal bootstrap nodes and enable the JSON-RPC APIs.\ndocker container run -p 8545:8545 statusim/nimbus-portal-client:amd64-master-latest --rpc --rpc-address:0.0.0.0\n</code></pre> <p>Note</p> <p>Port 8545 is published and <code>rpc-address</code> is set to the <code>ANY</code> address in this command to allow access to the JSON-RPC API from outside the Docker image. You might want to adjust that depending on the use case &amp; security model. It is also recommended to use a mounted volume for <code>nimbus_portal_client</code>'s <code>--data-dir</code> in case of a long-running container.</p>"},{"location":"quick-start-docker.html#try-requesting-an-execution-layer-block-from-the-network","title":"Try requesting an execution layer block from the network","text":"<p>Requesting history content on the Portal network can be easily tested by using the <code>portal_historyGetContent</code> JSON-RPC from the Portal JSON-RPC API.</p> <pre><code>CONTENTKEY=0x004e61bc0000000000 # Serialized content key for block body of block 12345678\n# Run this command to get this block body (unverified):\ncurl -s -X POST -H 'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"id\":\"1\",\"method\":\"portal_historyGetContent\",\"params\":[\"'${CONTENTKEY}'\"]}' http://localhost:8545\n</code></pre>"},{"location":"quick-start-windows.html","title":"Quick start - Windows","text":"<p>This page takes you through the steps of getting the Nimbus Portal node running on the public network.</p> <p>The guide assumes Windows is being used. For Linux/macOS users follow this tutorial.</p>"},{"location":"quick-start-windows.html#steps","title":"Steps","text":""},{"location":"quick-start-windows.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Developer tools (C compiler, Make, Bash, CMake, Git 2.9.4 or newer)</li> </ul> <p>If you need help installing these tools, you can consult our prerequisites page.</p> <p>Note</p> <p>To build the Nimbus Portal client on Windows, the MinGW-w64 build environment is recommended. The build commands in the rest of this page assume the MinGW build environment is used.</p>"},{"location":"quick-start-windows.html#build-the-nimbus-portal-client","title":"Build the Nimbus Portal client","text":"<pre><code>git clone https://github.com/status-im/nimbus-eth1.git\ncd nimbus-eth1\nmingw32-make nimbus_portal_client\n\n# Test if binary was successfully build by running the help command.\n./build/nimbus_portal_client --help\n</code></pre>"},{"location":"quick-start-windows.html#run-a-nimbus-portal-client-on-the-portal-network","title":"Run a Nimbus Portal client on the Portal network","text":"<p>To accept offered block bodies and receipts, the Portal node requires access to a web3 interface. Currently, it uses a non-standard method to retrieve headers from the execution layer (EL), which is only supported by Nimbus.</p> <pre><code># Connect to the Portal bootstrap nodes and enable the JSON-RPC APIs\n# nimbus_execution_client running with rpc interface at http://127.0.0.1:8545\n./build/nimbus_portal_client --rpc --web3-url:http://127.0.0.1:8545\n</code></pre> <p>The node can also operate in standalone mode (without an execution layer). In this mode, data retrieval from the network is possible, but storing new data is not supported.</p> <pre><code># Connect to the Portal bootstrap nodes and enable the JSON-RPC APIs\n./build/nimbus_portal_client --rpc\n</code></pre>"},{"location":"quick-start-windows.html#try-requesting-an-execution-layer-block-from-the-network","title":"Try requesting an execution layer block from the network","text":"<p>Requesting history content on the Portal network can be easily tested by using the <code>portal_historyGetContent</code> JSON-RPC from the Portal JSON-RPC API.</p> <pre><code>CONTENTKEY=0x004e61bc0000000000 # Serialized content key for block body of block 12345678\n# Run this command to get this block body (unverified):\ncurl -s -X POST -H 'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"id\":\"1\",\"method\":\"portal_historyGetContent\",\"params\":[\"'${CONTENTKEY}'\"]}' http://localhost:8545\n</code></pre>"},{"location":"quick-start-windows.html#update-and-rebuild-the-nimbus-portal-client","title":"Update and rebuild the Nimbus Portal client","text":"<p>In order to stay up to date you can pull the latest version from our master branch. There are currently released versions tagged.</p> <pre><code># From the nimbus-eth1 repository\ngit pull\n# To bring the git submodules up to date\nmingw32-make update\n\nmingw32-make nimbus_portal_client\n</code></pre>"},{"location":"quick-start.html","title":"Quick start - Linux/macOS","text":"<p>This page takes you through the steps of getting the Nimbus Portal node running on the public network.</p> <p>The guide assumes Linux or macOS is being used. For Windows users follow this tutorial.</p>"},{"location":"quick-start.html#steps","title":"Steps","text":""},{"location":"quick-start.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Developer tools (C compiler, Make, Bash, CMake, Git 2.9.4 or newer)</li> </ul> <p>If you need help installing these tools, you can consult our prerequisites page.</p>"},{"location":"quick-start.html#build-the-nimbus-portal-client","title":"Build the Nimbus Portal client","text":"<pre><code>git clone https://github.com/status-im/nimbus-eth1.git\ncd nimbus-eth1\nmake nimbus_portal_client\n\n# Test if binary was successfully build by running the help command.\n./build/nimbus_portal_client --help\n</code></pre>"},{"location":"quick-start.html#run-a-nimbus-portal-client-on-the-portal-network","title":"Run a Nimbus Portal client on the Portal network","text":"<p>To accept offered block bodies and receipts, the Portal node requires access to a web3 interface. Currently, it uses a non-standard method to retrieve headers from the execution layer (EL), which is only supported by Nimbus.</p> <pre><code># Connect to the Portal bootstrap nodes and enable the JSON-RPC APIs\n# nimbus_execution_client running with rpc interface at http://127.0.0.1:8545\n./build/nimbus_portal_client --rpc --web3-url:http://127.0.0.1:8545\n</code></pre> <p>The node can also operate in standalone mode (without an execution layer). In this mode, data retrieval from the network is possible, but storing new data is not supported.</p> <pre><code># Connect to the Portal bootstrap nodes and enable the JSON-RPC APIs\n./build/nimbus_portal_client --rpc\n</code></pre>"},{"location":"quick-start.html#try-requesting-an-execution-layer-block-from-the-network","title":"Try requesting an execution layer block from the network","text":"<p>Requesting history content on the Portal network can be easily tested by using the <code>portal_historyGetContent</code> JSON-RPC from the Portal JSON-RPC API.</p> <pre><code>CONTENTKEY=0x004e61bc0000000000 # Serialized content key for block body of block 12345678\n# Run this command to get this block body (unverified):\ncurl -s -X POST -H 'Content-Type: application/json' -d '{\"jsonrpc\":\"2.0\",\"id\":\"1\",\"method\":\"portal_historyGetContent\",\"params\":[\"'${CONTENTKEY}'\"]}' http://localhost:8545\n</code></pre>"},{"location":"quick-start.html#update-and-rebuild-the-nimbus-portal-client","title":"Update and rebuild the Nimbus Portal client","text":"<p>In order to stay up to date you can pull the latest version from our master branch. There are currently released versions tagged.</p> <pre><code># From the nimbus-eth1 repository\ngit pull\n# To bring the git submodules up to date\nmake update\n\nmake nimbus_portal_client\n</code></pre>"},{"location":"run-local-testnet.html","title":"Running a local testnet","text":"<p>To easily start a local testnet you can use the <code>launch_local_testnet.sh</code> script. This script allows you to start <code>n</code> amount of nodes and then run several actions on them through the JSON-RPC API.</p>"},{"location":"run-local-testnet.html#run-the-local-testnet-script","title":"Run the local testnet script","text":"<pre><code># Run the script, default start 3 nodes\n./portal/scripts/launch_local_testnet.sh\n# Run the script with 16 nodes\n./portal/scripts/launch_local_testnet.sh -n 16\n\n# See the script help\n./portal/scripts/launch_local_testnet.sh --help\n</code></pre> <p>The nodes will be started and all nodes will use <code>node0</code> as bootstrap node.</p> <p>The <code>data-dir</code>s and logs of each node can be found in <code>./local_testnet_data/</code>.</p> <p>You can manually start extra nodes that connect to the network by providing any of the running nodes their ENR.</p> <p>E.g. to manually add a Portal node to the local testnet run:</p> <pre><code>./build/nimbus_portal_client --rpc --network:none --udp-port:9010 --nat:extip:127.0.0.1 --bootstrap-node:`cat ./local_testnet_data/node0/portal_node.enr`\n</code></pre>"},{"location":"test-suite.html","title":"Nimbus Portal test suite","text":""},{"location":"test-suite.html#run-the-nimbus-portal-test-suite","title":"Run the Nimbus Portal test suite","text":"<pre><code># From the nimbus-eth1 repository\nmake portal-test\n</code></pre>"},{"location":"test-suite.html#run-nimbus-portal-local-testnet-script","title":"Run Nimbus Portal local testnet script","text":"<pre><code>./portal/scripts/launch_local_testnet.sh --run-tests\n</code></pre> <p>Find more details on the usage and workings of the local testnet script here.</p>"},{"location":"testnet-beacon-network.html","title":"Testing beacon network on local testnet","text":"<p>This section explains how one can set up a local testnet together with a beacon network bridge in order to test if all nodes can do the beacon light client sync and stay up to date with the latest head of the chain.</p> <p>To accomodate this, the <code>launch_local_testnet.sh</code> script has the option to launch the <code>nimbus_portal_bridge</code> automatically and connect it to <code>node0</code> of the local tesnet.</p>"},{"location":"testnet-beacon-network.html#run-the-local-testnet-script-with-bridge","title":"Run the local testnet script with bridge","text":"<p>The <code>launch_local_testnet.sh</code> script must be launched with the <code>--trusted-block-root</code> cli option. The individual nodes will be started with this <code>trusted-block-root</code> and each node will try to start sync from this block root.</p> <p>Run the following command to launch the network with the <code>nimbus_portal_bridge</code> activated for the beacon network.</p> <pre><code>TRUSTED_BLOCK_ROOT=0x1234567890123456789012345678901234567890123456789012345678901234 # Replace with trusted block root.\n\n# Run the script, start 8 nodes + nimbus_portal_bridge\n./portal/scripts/launch_local_testnet.sh -n8 --trusted-block-root ${TRUSTED_BLOCK_ROOT} --portal-bridge\n</code></pre>"},{"location":"testnet-beacon-network.html#run-the-local-testnet-script-and-launch-the-bridge-manually","title":"Run the local testnet script and launch the bridge manually","text":"<p>To have control over when to start or restart the <code>nimbus_portal_bridge</code> on can also control the bridge manually, e.g. start the testnet:</p> <pre><code>TRUSTED_BLOCK_ROOT=0x1234567890123456789012345678901234567890123456789012345678901234 # Replace with trusted block root.\n\n# Run the script, start 8 nodes\n./portal/scripts/launch_local_testnet.sh -n8 --trusted-block-root ${TRUSTED_BLOCK_ROOT}\n</code></pre> <p>Next, build and run the <code>nimbus_portal_bridge</code> for the beacon network:</p> <pre><code>make nimbus_portal_bridge\n\n# --rpc-port 10000 = default node0\n# --rest-url = access to beacon node API, default http://127.0.0.1:5052\n./build/nimbus_portal_bridge beacon --trusted-block-root:${TRUSTED_BLOCK_ROOT} --rest-url:http://127.0.0.1:5052 --backfill-amount:128 --portal-rpc-url:http://127.0.0.1:10000\n</code></pre>"},{"location":"testnet-history-network.html","title":"Testing history network on local testnet","text":"<p>There is an automated test for the Portal history network integrated in the <code>launch_local_testnet.sh</code> script.</p> <p>The <code>test_portal_testnet</code> binary can be run from within this script and do a set of actions on the nodes through the JSON-RPC API. When that is finished, all nodes will be killed.</p>"},{"location":"testnet-history-network.html#run-the-local-testnet-script-with-history-network-test","title":"Run the local testnet script with history network test","text":"<pre><code># Run the script, default start 64 nodes and run history tests\n./portal/scripts/launch_local_testnet.sh --run-tests -n64\n</code></pre>"},{"location":"testnet-history-network.html#details-of-the-test_portal_testnet-test","title":"Details of the <code>test_portal_testnet</code> test","text":"<p>Following steps are done:</p> <ol> <li>Nodes join the network by providing them all with one and the same   bootstrap node at start-up.</li> <li>Attempt to add the ENRs of all the nodes to each node its routing table:   This is done in order to quickly simulate a network that has all the nodes   propagated around. The JSON-RPC <code>portal_historyAddEnr</code> is used for this.</li> <li>Select, at random, a node id of one of the nodes. Let every node do a   lookup for this node id.   This is done to validate that every node can successfully lookup a specific   node in the DHT.</li> </ol>"},{"location":"upgrade.html","title":"Upgrade","text":"<p>To upgrade to the latest version you need to update the nimbus-eth1 repository and re-compile the Nimbus Portal client.</p> <p>Note</p> <p>In this state of development there are no official releases yet nor git tags for different versions.</p>"},{"location":"upgrade.html#upgrade-to-the-latest-version","title":"Upgrade to the latest version","text":"<p>Upgrading the Nimbus Portal client when built from source is similar to the installation process.</p> <p>Run:</p> <pre><code># Download the updated source code\ngit pull &amp;&amp; make update\n\n# Build from the newly updated source\nmake -j4 nimbus_portal_client\n</code></pre> <p>Complete the upgrade by restarting the node.</p> <p>Tip</p> <p>To check which version of the Nimbus Portal client you're currently running, run <code>./build/nimbus_portal_client --version</code></p>"}]}